{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32e00c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vigha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import math\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "no_of_datasets = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ee0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#To read data from the folders\n",
    "train_data_folders = [\"\\\\enron1_train\\\\enron1\\\\train\", \"\\\\enron2_train\\\\train\", \"\\\\enron4_train\\\\enron4\\\\train\"]\n",
    "test_data_folders = [\"\\\\enron1_test\\\\enron1\\\\test\", \"\\\\enron2_test\\\\test\", \"\\\\enron4_test\\\\enron4\\\\test\"]\n",
    "\n",
    "train_data_folders=[ os.getcwd() + t for t in train_data_folders]\n",
    "test_data_folders=[ os.getcwd() + t for t in test_data_folders]\n",
    "\n",
    "\n",
    "#0, 1 ,2 indices of list will be for the 3 datasets\n",
    "#Each list is vocabulary for i+1th dataset \n",
    "train_vocab_lists = [[] for i in range(no_of_datasets)]\n",
    "#counter to help creating vector of words later\n",
    "no_message = []\n",
    "for dataset, train_folder in enumerate(train_data_folders):\n",
    "    #-1 is for Subject which I am removing\n",
    "    count = 0\n",
    "    for filename in os.listdir(train_folder + \"\\\\ham\"):\n",
    "        count += 1\n",
    "        with open(os.path.join(train_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    if word not in train_vocab_lists[dataset]:\n",
    "                        train_vocab_lists[dataset].append(word)\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(train_folder + \"\\\\spam\"):\n",
    "        count += 1\n",
    "        with open(os.path.join(train_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    if word not in train_vocab_lists[dataset]:\n",
    "                        train_vocab_lists[dataset].append(word)\n",
    "    no_message.append(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f1bf4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the BAG OF WORD MODEL\n",
      "The features x examples matrices are: \n",
      "Matrix for dataset 1 [[1. 6. 3. ... 0. 0. 0.]\n",
      " [1. 3. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 9. 0. ... 0. 0. 1.]\n",
      " [1. 3. 0. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]]\n",
      "Matrix for dataset 2 [[ 1. 19.  2. ...  0.  0.  0.]\n",
      " [ 1. 11.  0. ...  0.  0.  0.]\n",
      " [ 1.  1.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 1.  1.  0. ...  0.  0.  1.]\n",
      " [ 1.  1.  0. ...  0.  0.  1.]\n",
      " [ 1.  6.  0. ...  1.  2.  1.]]\n",
      "Matrix for dataset 3 [[1. 7. 3. ... 0. 0. 0.]\n",
      " [1. 2. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 2. 0. ... 1. 1. 1.]\n",
      " [1. 2. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "bow_matrices = []\n",
    "\n",
    "test_bow_matrices = []\n",
    "\n",
    "#iterating over number of messages to create bag of word matrices for all 3 datasets\n",
    "#Last column of the numpy array is the inference i.e 0 for ham and 1 for spam\n",
    "for i in range(no_of_datasets):\n",
    "    bow_matrices.append(np.zeros((no_message[i], len(train_vocab_lists[i])+1)))\n",
    "\n",
    "for i in range(no_of_datasets):\n",
    "    test_bow_matrices.append(np.zeros((no_message[i], len(train_vocab_lists[i])+1)))\n",
    "\n",
    "\n",
    "for dataset, train_folder in enumerate(train_data_folders):\n",
    "    msg_index = 0\n",
    "    for filename in os.listdir(train_folder + \"\\\\ham\"): \n",
    "        with open(os.path.join(train_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    (bow_matrices[dataset])[msg_index][train_vocab_lists[dataset].index(word)] += 1\n",
    "                    (bow_matrices[dataset])[msg_index][-1] = 0\n",
    "            msg_index += 1\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(train_folder + \"\\\\spam\"):\n",
    "        with open(os.path.join(train_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    (bow_matrices[dataset])[msg_index][train_vocab_lists[dataset].index(word)] += 1\n",
    "                    (bow_matrices[dataset])[msg_index][-1] = 1\n",
    "            msg_index += 1\n",
    "\n",
    "for dataset, test_folder in enumerate(test_data_folders):\n",
    "    msg_index = 0\n",
    "    for filename in os.listdir(test_folder + \"\\\\ham\"): \n",
    "        with open(os.path.join(test_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    try:\n",
    "                        (test_bow_matrices[dataset])[msg_index][train_vocab_lists[dataset].index(word)] += 1\n",
    "                        (test_bow_matrices[dataset])[msg_index][-1] = 0\n",
    "                    except:\n",
    "                        pass\n",
    "            msg_index += 1\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(test_folder + \"\\\\spam\"):\n",
    "        with open(os.path.join(test_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    try:\n",
    "                        (test_bow_matrices[dataset])[msg_index][train_vocab_lists[dataset].index(word)] += 1\n",
    "                        (test_bow_matrices[dataset])[msg_index][-1] = 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "            msg_index += 1\n",
    "\n",
    "print(\"Using the BAG OF WORD MODEL\")\n",
    "print(\"The features x examples matrices are: \")\n",
    "\n",
    "print(\"Matrix for dataset 1\", bow_matrices[0])\n",
    "print(\"Matrix for dataset 2\", bow_matrices[1])\n",
    "print(\"Matrix for dataset 3\", bow_matrices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a2aa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the BERNOULLI MODEL\n",
      "The features x examples matrices are: \n",
      "Matrix for dataset 1 [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]]\n",
      "Matrix for dataset 2 [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]]\n",
      "Matrix for dataset 3 [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "bnouli_matrices = []\n",
    "\n",
    "test_bnouli_matrices = []\n",
    "\n",
    "#iterating over number of messages to create bag of word matrices for all 3 datasets\n",
    "#Last column of the numpy array is the inference i.e 0 for ham and 1 for spam\n",
    "for i in range(no_of_datasets):\n",
    "    bnouli_matrices.append(np.zeros((no_message[i], len(train_vocab_lists[i])+1)))\n",
    "\n",
    "for i in range(no_of_datasets):\n",
    "    test_bnouli_matrices.append(np.zeros((no_message[i], len(train_vocab_lists[i])+1)))\n",
    "\n",
    "\n",
    "for dataset_index, train_folder in enumerate(train_data_folders):\n",
    "    msg_index = 0\n",
    "    for filename in os.listdir(train_folder + \"\\\\ham\"): \n",
    "        with open(os.path.join(train_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    (bnouli_matrices[dataset_index])[msg_index][train_vocab_lists[dataset_index].index(word)] = 1\n",
    "                    (bnouli_matrices[dataset_index])[msg_index][-1] = 0\n",
    "            msg_index += 1\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(train_folder + \"\\\\spam\"):\n",
    "        with open(os.path.join(train_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    (bnouli_matrices[dataset_index])[msg_index][train_vocab_lists[dataset_index].index(word)] = 1\n",
    "                    (bnouli_matrices[dataset_index])[msg_index][-1] = 1\n",
    "            msg_index += 1\n",
    "\n",
    "for dataset_index, test_folder in enumerate(test_data_folders):\n",
    "    msg_index = 0\n",
    "    for filename in os.listdir(test_folder + \"\\\\ham\"): \n",
    "        with open(os.path.join(test_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    try:\n",
    "                        (test_bnouli_matrices[dataset_index])[msg_index][train_vocab_lists[dataset_index].index(word)] = 1\n",
    "                        (test_bnouli_matrices[dataset_index])[msg_index][-1] = 0\n",
    "                    except:\n",
    "                        pass\n",
    "            msg_index += 1\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(test_folder + \"\\\\spam\"):\n",
    "        with open(os.path.join(test_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    try:\n",
    "                        (test_bnouli_matrices[dataset_index])[msg_index][train_vocab_lists[dataset_index].index(word)] = 1\n",
    "                        (test_bnouli_matrices[dataset_index])[msg_index][-1] = 1\n",
    "                    except:\n",
    "                        pass\n",
    "            msg_index += 1\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Using the BERNOULLI MODEL\")\n",
    "print(\"The features x examples matrices are: \")\n",
    "\n",
    "print(\"Matrix for dataset 1\", bnouli_matrices[0])\n",
    "print(\"Matrix for dataset 2\", bnouli_matrices[1])\n",
    "print(\"Matrix for dataset 3\", bnouli_matrices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fddecb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(weights, X_train, Y_train, lambda_reg, lr, iterations):\n",
    "    for iter in range(iterations):\n",
    "        py1_xi = np.zeros(len(Y_train))\n",
    "        for i in range(len(X_train)):\n",
    "            wsum = weights[0] + np.dot(weights[1:], X_train[i]) \n",
    "            py1_xi[i] = expit(wsum)\n",
    "        for i in range(len(weights)):\n",
    "            if i == 0:\n",
    "                weights[i] = weights[i]*(1 - lr*lambda_reg)\n",
    "                for j in range(len(Y_train)):\n",
    "                    weights[i] += lr*(Y_train[j][0] - py1_xi[j])\n",
    "            else:\n",
    "                weights[i] = weights[i]*(1 - lr*lambda_reg)\n",
    "                for j in range(len(Y_train)):\n",
    "                    weights[i] += lr*X_train[j][i-1]*(Y_train[j][0] - py1_xi[j])\n",
    "                \n",
    "def validate_model(weights, X_val, Y_val, lambda_reg, lr):\n",
    "    total_preds = len(Y_val)\n",
    "    correct_preds = 0\n",
    "    for i in range(len(X_val)):\n",
    "        wsum = weights[0] + np.dot(weights[1:], X_val[i])\n",
    "\n",
    "        if wsum > 0 and Y_val[i][0] == 1:\n",
    "            correct_preds += 1\n",
    "        elif wsum <= 0 and Y_val[i][0] == 0:\n",
    "            correct_preds += 1\n",
    "    \n",
    "    print(\"Total values in validation set: \",total_preds)\n",
    "    print(\"Correct predictions in validation set: \", correct_preds)\n",
    "    print(\"Accuracy when (lambda, learning rate) is\", lambda_reg, lr ,\": \", correct_preds*100/total_preds)\n",
    "\n",
    "def test_model(weights, X_val, Y_val):\n",
    "        Y_test = [y[0] for y in Y_val]\n",
    "        predictions = [0]*len(Y_test)\n",
    "        for i in range(len(X_val)):\n",
    "            wsum = weights[0] + np.dot(weights[1:], X_val[i])\n",
    "            if wsum > 0:\n",
    "                predictions[i] = 1\n",
    "            elif wsum <= 0:\n",
    "                predictions[i] = 0\n",
    "        \n",
    "        accuracy = accuracy_score(Y_test, predictions)\n",
    "        precision = precision_score(Y_test, predictions)\n",
    "        recall = recall_score(Y_test, predictions)\n",
    "        f1 = f1_score(Y_test, predictions)\n",
    "\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 Score:\", f1)\n",
    "\n",
    "\n",
    "def experiment_model_hyperparas(lambda_ranges, X_train, Y_train, X_val, Y_val, learning_rates, iter_hard_limits):\n",
    "    for lambda_reg in lambda_ranges:\n",
    "        for lr in learning_rates:\n",
    "            for iterations in iter_hard_limits:\n",
    "                np.random.seed(31234)\n",
    "                weights = np.random.uniform(-1, 1, len(train_vocab_lists[dataset])+1)\n",
    "                print(\"------------------------------------------------\")\n",
    "                print(\"Training weights for \", iterations, \" iterations.\")\n",
    "                print(\"----------------------------------------------\")\n",
    "                train_model(weights, X_train, Y_train, lambda_reg, lr, iterations)\n",
    "                validate_model(weights, X_val, Y_val, lambda_reg, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5c5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------dataset:  1 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  119\n",
      "Accuracy when (lambda, learning rate) is 0.0001 0.1 :  88.14814814814815\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  119\n",
      "Accuracy when (lambda, learning rate) is 0.001 0.1 :  88.14814814814815\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  119\n",
      "Accuracy when (lambda, learning rate) is 0.01 0.1 :  88.14814814814815\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  121\n",
      "Accuracy when (lambda, learning rate) is 0.1 0.1 :  89.62962962962963\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  96\n",
      "Accuracy when (lambda, learning rate) is 1.0 0.1 :  71.11111111111111\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  127\n",
      "Accuracy when (lambda, learning rate) is 0.0001 0.1 :  94.07407407407408\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  127\n",
      "Accuracy when (lambda, learning rate) is 0.001 0.1 :  94.07407407407408\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  127\n",
      "Accuracy when (lambda, learning rate) is 0.01 0.1 :  94.07407407407408\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  127\n",
      "Accuracy when (lambda, learning rate) is 0.1 0.1 :  94.07407407407408\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  135\n",
      "Correct predictions in validation set:  107\n",
      "Accuracy when (lambda, learning rate) is 1.0 0.1 :  79.25925925925925\n",
      "------------------------------dataset:  2 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  130\n",
      "Accuracy when (lambda, learning rate) is 0.0001 0.1 :  93.5251798561151\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  129\n",
      "Accuracy when (lambda, learning rate) is 0.001 0.1 :  92.80575539568345\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  129\n",
      "Accuracy when (lambda, learning rate) is 0.01 0.1 :  92.80575539568345\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  130\n",
      "Accuracy when (lambda, learning rate) is 0.1 0.1 :  93.5251798561151\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  108\n",
      "Accuracy when (lambda, learning rate) is 1.0 0.1 :  77.6978417266187\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  133\n",
      "Accuracy when (lambda, learning rate) is 0.0001 0.1 :  95.68345323741008\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  133\n",
      "Accuracy when (lambda, learning rate) is 0.001 0.1 :  95.68345323741008\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  133\n",
      "Accuracy when (lambda, learning rate) is 0.01 0.1 :  95.68345323741008\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  133\n",
      "Accuracy when (lambda, learning rate) is 0.1 0.1 :  95.68345323741008\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  139\n",
      "Correct predictions in validation set:  128\n",
      "Accuracy when (lambda, learning rate) is 1.0 0.1 :  92.0863309352518\n",
      "------------------------------dataset:  3 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  155\n",
      "Accuracy when (lambda, learning rate) is 0.0001 0.1 :  96.27329192546584\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  155\n",
      "Accuracy when (lambda, learning rate) is 0.001 0.1 :  96.27329192546584\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  156\n",
      "Accuracy when (lambda, learning rate) is 0.01 0.1 :  96.8944099378882\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  155\n",
      "Accuracy when (lambda, learning rate) is 0.1 0.1 :  96.27329192546584\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  123\n",
      "Accuracy when (lambda, learning rate) is 1.0 0.1 :  76.3975155279503\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  155\n",
      "Accuracy when (lambda, learning rate) is 0.0001 0.1 :  96.27329192546584\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  155\n",
      "Accuracy when (lambda, learning rate) is 0.001 0.1 :  96.27329192546584\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  155\n",
      "Accuracy when (lambda, learning rate) is 0.01 0.1 :  96.27329192546584\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  155\n",
      "Accuracy when (lambda, learning rate) is 0.1 0.1 :  96.27329192546584\n",
      "------------------------------------------------\n",
      "Training weights for  100  iterations.\n",
      "----------------------------------------------\n",
      "Total values in validation set:  161\n",
      "Correct predictions in validation set:  154\n",
      "Accuracy when (lambda, learning rate) is 1.0 0.1 :  95.65217391304348\n"
     ]
    }
   ],
   "source": [
    "for dataset in range(no_of_datasets):\n",
    "    print(\"------------------------------dataset: \", dataset + 1, \"--------------------------------\")\n",
    "    train_matrices = [bow_matrices, bnouli_matrices]\n",
    "    test_matrices = [test_bow_matrices, test_bnouli_matrices]\n",
    "    for i in range(2):\n",
    "        data_matrix = train_matrices[i][dataset].copy()\n",
    "        if i == 0:\n",
    "            print(\"----------------------------LOR with BOW MODEL------------------------\")\n",
    "        else:\n",
    "            print(\"---------------------------LOR with Bernoulli Model----------------------\")\n",
    "        #For the experimentation\n",
    "        lambda_ranges = [0.01, 0.1, 1, 10]\n",
    "        learning_rates = [0.01, 0.1, 1.0, 10]\n",
    "        iter_hard_limits = [10, 20, 50, 100, 250]\n",
    "        seed_r = 1234\n",
    "\n",
    "        np.random.seed(seed_r)\n",
    "        np.random.shuffle(data_matrix)\n",
    "\n",
    "        partition_70 = int(math.floor(0.7*len(data_matrix)))\n",
    "        train_set = data_matrix[:partition_70]\n",
    "        val_set = data_matrix[partition_70:]\n",
    "\n",
    "        X_train = train_set[:,:-1]\n",
    "        Y_train = train_set[:, len(train_set[0])-1:]\n",
    "\n",
    "        X_val = val_set[:,:-1]\n",
    "        Y_val = val_set[:, len(val_set[0])-1:]\n",
    "\n",
    "\n",
    "        \n",
    "        #Just for experimentation (Realised it is like grid-search)\n",
    "        #Uncomment these to experiment\n",
    "\n",
    "        \n",
    "        # lambda_ranges = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "        # lr = [0.1]\n",
    "        # iters = [100]\n",
    "        # experiment_model_hyperparas(lambda_ranges, X_train, Y_train, X_val, Y_val, lr, iters)\n",
    "        \n",
    "        #Will take lambda as 0.1, learning rate as 0.1 and 250 iterations, based on the calulations above  \n",
    "\n",
    "        #The training set for all of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93fb731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------dataset:  1 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "------------------------------------------------\n",
      "Training final weights all data for dataset :  1  and  250  iterations.\n",
      "----------------------------------------------\n",
      "Accuracy: 0.92\n",
      "Precision: 0.8543046357615894\n",
      "Recall: 0.9020979020979021\n",
      "F1 Score: 0.8775510204081632\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "------------------------------------------------\n",
      "Training final weights all data for dataset :  1  and  250  iterations.\n",
      "----------------------------------------------\n",
      "Accuracy: 0.9444444444444444\n",
      "Precision: 0.8986486486486487\n",
      "Recall: 0.9300699300699301\n",
      "F1 Score: 0.9140893470790378\n",
      "------------------------------dataset:  2 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "------------------------------------------------\n",
      "Training final weights all data for dataset :  2  and  250  iterations.\n",
      "----------------------------------------------\n",
      "Accuracy: 0.9200863930885529\n",
      "Precision: 0.8421052631578947\n",
      "Recall: 0.8347826086956521\n",
      "F1 Score: 0.8384279475982532\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "------------------------------------------------\n",
      "Training final weights all data for dataset :  2  and  250  iterations.\n",
      "----------------------------------------------\n",
      "Accuracy: 0.9568034557235421\n",
      "Precision: 0.905982905982906\n",
      "Recall: 0.9217391304347826\n",
      "F1 Score: 0.9137931034482759\n",
      "------------------------------dataset:  3 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "------------------------------------------------\n",
      "Training final weights all data for dataset :  3  and  250  iterations.\n",
      "----------------------------------------------\n",
      "Accuracy: 0.9551401869158879\n",
      "Precision: 0.9686684073107049\n",
      "Recall: 0.9686684073107049\n",
      "F1 Score: 0.9686684073107049\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "------------------------------------------------\n",
      "Training final weights all data for dataset :  3  and  250  iterations.\n",
      "----------------------------------------------\n",
      "Accuracy: 0.9794392523364486\n",
      "Precision: 0.9720812182741116\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9858429858429858\n"
     ]
    }
   ],
   "source": [
    "for dataset in range(no_of_datasets):\n",
    "    print(\"------------------------------dataset: \", dataset + 1, \"--------------------------------\")\n",
    "    train_matrices = [bow_matrices, bnouli_matrices]\n",
    "    test_matrices = [test_bow_matrices, test_bnouli_matrices]\n",
    "    for i in range(2):\n",
    "        data_matrix = train_matrices[i][dataset].copy()\n",
    "        if i == 0:\n",
    "            print(\"----------------------------LOR with BOW MODEL------------------------\")\n",
    "        else:\n",
    "            print(\"---------------------------LOR with Bernoulli Model----------------------\")\n",
    "\n",
    "        X_train = data_matrix[:,:-1]\n",
    "        Y_train = data_matrix[:, len(data_matrix[0])-1:]\n",
    "\n",
    "\n",
    "        learning_rate = 0.1\n",
    "        lambda_val = 0.1\n",
    "        iterations = 250\n",
    "\n",
    "        weights = np.random.uniform(-1, 1, len(train_vocab_lists[dataset])+1)\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(\"Training final weights all data for dataset : \", dataset + 1, \" and \", iterations, \" iterations.\")\n",
    "        print(\"----------------------------------------------\")\n",
    "        train_model(weights, X_train, Y_train, lambda_val, learning_rate, iterations)\n",
    "        \n",
    "        \n",
    "        test_matrix = test_matrices[i][dataset].copy()\n",
    "\n",
    "        X_test = test_matrix[:,:-1]\n",
    "        Y_test = test_matrix[:, len(test_matrix[0])-1:]\n",
    "\n",
    "        test_model(weights, X_test, Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
