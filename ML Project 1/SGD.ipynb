{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32e00c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vigha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# enron1 is 1st, enron2 is 2nd and enron4 is 3rd\n",
    "no_of_datasets = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ee0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#To read data from the folders\n",
    "train_data_folders = [\"\\\\enron1_train\\\\enron1\\\\train\", \"\\\\enron2_train\\\\train\", \"\\\\enron4_train\\\\enron4\\\\train\"]\n",
    "test_data_folders = [\"\\\\enron1_test\\\\enron1\\\\test\", \"\\\\enron2_test\\\\test\", \"\\\\enron4_test\\\\enron4\\\\test\"]\n",
    "\n",
    "train_data_folders=[ os.getcwd() + t for t in train_data_folders]\n",
    "test_data_folders=[ os.getcwd() + t for t in test_data_folders]\n",
    "\n",
    "\n",
    "#0, 1 ,2 indices of list will be for the 3 datasets\n",
    "#Each list is vocabulary for i+1th dataset \n",
    "train_vocab_lists = [[] for i in range(no_of_datasets)]\n",
    "#counter to help creating vector of words later\n",
    "no_message = []\n",
    "for dataset, train_folder in enumerate(train_data_folders):\n",
    "    #-1 is for Subject which I am removing\n",
    "    count = 0\n",
    "    for filename in os.listdir(train_folder + \"\\\\ham\"):\n",
    "        count += 1\n",
    "        with open(os.path.join(train_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    if word not in train_vocab_lists[dataset]:\n",
    "                        train_vocab_lists[dataset].append(word)\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(train_folder + \"\\\\spam\"):\n",
    "        count += 1\n",
    "        with open(os.path.join(train_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    if word not in train_vocab_lists[dataset]:\n",
    "                        train_vocab_lists[dataset].append(word)\n",
    "    no_message.append(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1bf4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the BAG OF WORD MODEL\n",
      "The features x examples matrices are: \n",
      "Matrix for dataset 1 [[1. 6. 3. ... 0. 0. 0.]\n",
      " [1. 3. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 9. 0. ... 0. 0. 1.]\n",
      " [1. 3. 0. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]]\n",
      "Matrix for dataset 2 [[ 1. 19.  2. ...  0.  0.  0.]\n",
      " [ 1. 11.  0. ...  0.  0.  0.]\n",
      " [ 1.  1.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 1.  1.  0. ...  0.  0.  1.]\n",
      " [ 1.  1.  0. ...  0.  0.  1.]\n",
      " [ 1.  6.  0. ...  1.  2.  1.]]\n",
      "Matrix for dataset 3 [[1. 7. 3. ... 0. 0. 0.]\n",
      " [1. 2. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 2. 0. ... 1. 1. 1.]\n",
      " [1. 2. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "bow_matrices = []\n",
    "\n",
    "test_bow_matrices = []\n",
    "\n",
    "#iterating over number of messages to create bag of word matrices for all 3 datasets\n",
    "#Last column of the numpy array is the inference i.e 0 for ham and 1 for spam\n",
    "for i in range(no_of_datasets):\n",
    "    bow_matrices.append(np.zeros((no_message[i], len(train_vocab_lists[i])+1)))\n",
    "\n",
    "for i in range(no_of_datasets):\n",
    "    test_bow_matrices.append(np.zeros((no_message[i], len(train_vocab_lists[i])+1)))\n",
    "\n",
    "\n",
    "for dataset, train_folder in enumerate(train_data_folders):\n",
    "    msg_index = 0\n",
    "    for filename in os.listdir(train_folder + \"\\\\ham\"): \n",
    "        with open(os.path.join(train_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    (bow_matrices[dataset])[msg_index][train_vocab_lists[dataset].index(word)] += 1\n",
    "                    (bow_matrices[dataset])[msg_index][-1] = 0\n",
    "            msg_index += 1\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(train_folder + \"\\\\spam\"):\n",
    "        with open(os.path.join(train_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    (bow_matrices[dataset])[msg_index][train_vocab_lists[dataset].index(word)] += 1\n",
    "                    (bow_matrices[dataset])[msg_index][-1] = 1\n",
    "            msg_index += 1\n",
    "\n",
    "for dataset, test_folder in enumerate(test_data_folders):\n",
    "    msg_index = 0\n",
    "    for filename in os.listdir(test_folder + \"\\\\ham\"): \n",
    "        with open(os.path.join(test_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    try:\n",
    "                        (test_bow_matrices[dataset])[msg_index][train_vocab_lists[dataset].index(word)] += 1\n",
    "                        (test_bow_matrices[dataset])[msg_index][-1] = 0\n",
    "                    except:\n",
    "                        pass\n",
    "            msg_index += 1\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(test_folder + \"\\\\spam\"):\n",
    "        with open(os.path.join(test_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    try:\n",
    "                        (test_bow_matrices[dataset])[msg_index][train_vocab_lists[dataset].index(word)] += 1\n",
    "                        (test_bow_matrices[dataset])[msg_index][-1] = 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "            msg_index += 1\n",
    "\n",
    "print(\"Using the BAG OF WORD MODEL\")\n",
    "print(\"The features x examples matrices are: \")\n",
    "\n",
    "print(\"Matrix for dataset 1\", bow_matrices[0])\n",
    "print(\"Matrix for dataset 2\", bow_matrices[1])\n",
    "print(\"Matrix for dataset 3\", bow_matrices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a2aa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the BERNOULLI MODEL\n",
      "The features x examples matrices are: \n",
      "Matrix for dataset 1 [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]]\n",
      "Matrix for dataset 2 [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]]\n",
      "Matrix for dataset 3 [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "bnouli_matrices = []\n",
    "\n",
    "test_bnouli_matrices = []\n",
    "\n",
    "#iterating over number of messages to create bag of word matrices for all 3 datasets\n",
    "#Last column of the numpy array is the inference i.e 0 for ham and 1 for spam\n",
    "for i in range(no_of_datasets):\n",
    "    bnouli_matrices.append(np.zeros((no_message[i], len(train_vocab_lists[i])+1)))\n",
    "\n",
    "for i in range(no_of_datasets):\n",
    "    test_bnouli_matrices.append(np.zeros((no_message[i], len(train_vocab_lists[i])+1)))\n",
    "\n",
    "\n",
    "for dataset_index, train_folder in enumerate(train_data_folders):\n",
    "    msg_index = 0\n",
    "    for filename in os.listdir(train_folder + \"\\\\ham\"): \n",
    "        with open(os.path.join(train_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    (bnouli_matrices[dataset_index])[msg_index][train_vocab_lists[dataset_index].index(word)] = 1\n",
    "                    (bnouli_matrices[dataset_index])[msg_index][-1] = 0\n",
    "            msg_index += 1\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(train_folder + \"\\\\spam\"):\n",
    "        with open(os.path.join(train_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    (bnouli_matrices[dataset_index])[msg_index][train_vocab_lists[dataset_index].index(word)] = 1\n",
    "                    (bnouli_matrices[dataset_index])[msg_index][-1] = 1\n",
    "            msg_index += 1\n",
    "\n",
    "for dataset_index, test_folder in enumerate(test_data_folders):\n",
    "    msg_index = 0\n",
    "    for filename in os.listdir(test_folder + \"\\\\ham\"): \n",
    "        with open(os.path.join(test_folder + \"\\\\ham\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    try:\n",
    "                        (test_bnouli_matrices[dataset_index])[msg_index][train_vocab_lists[dataset_index].index(word)] = 1\n",
    "                        (test_bnouli_matrices[dataset_index])[msg_index][-1] = 0\n",
    "                    except:\n",
    "                        pass\n",
    "            msg_index += 1\n",
    "                \n",
    "                    \n",
    "    for filename in os.listdir(test_folder + \"\\\\spam\"):\n",
    "        with open(os.path.join(test_folder + \"\\\\spam\\\\\" + filename), 'r', errors='ignore') as f:\n",
    "            for sent in f.readlines():\n",
    "                for word in nltk.tokenize.word_tokenize(sent):\n",
    "                    try:\n",
    "                        (test_bnouli_matrices[dataset_index])[msg_index][train_vocab_lists[dataset_index].index(word)] = 1\n",
    "                        (test_bnouli_matrices[dataset_index])[msg_index][-1] = 1\n",
    "                    except:\n",
    "                        pass\n",
    "            msg_index += 1\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Using the BERNOULLI MODEL\")\n",
    "print(\"The features x examples matrices are: \")\n",
    "\n",
    "print(\"Matrix for dataset 1\", bnouli_matrices[0])\n",
    "print(\"Matrix for dataset 2\", bnouli_matrices[1])\n",
    "print(\"Matrix for dataset 3\", bnouli_matrices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6e9efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------dataset:  0 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "Best parameters: {'alpha': 0.001, 'learning_rate': 'adaptive', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n",
      "Accuracy: 0.8933333333333333\n",
      "Precision: 0.8145695364238411\n",
      "Recall: 0.8601398601398601\n",
      "F1 Score: 0.8367346938775511\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "Best parameters: {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2'}\n",
      "Accuracy: 0.9555555555555556\n",
      "Precision: 0.9072847682119205\n",
      "Recall: 0.958041958041958\n",
      "F1 Score: 0.9319727891156463\n",
      "------------------------------dataset:  1 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "Best parameters: {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n",
      "Accuracy: 0.9071274298056156\n",
      "Precision: 0.78125\n",
      "Recall: 0.8695652173913043\n",
      "F1 Score: 0.823045267489712\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "Best parameters: {'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n",
      "Accuracy: 0.9546436285097192\n",
      "Precision: 0.8983050847457628\n",
      "Recall: 0.9217391304347826\n",
      "F1 Score: 0.9098712446351931\n",
      "------------------------------dataset:  2 --------------------------------\n",
      "----------------------------LOR with BOW MODEL------------------------\n",
      "Best parameters: {'alpha': 0.001, 'learning_rate': 'constant', 'loss': 'perceptron', 'max_iter': 100, 'penalty': 'l2'}\n",
      "Accuracy: 0.9570093457943926\n",
      "Precision: 0.96875\n",
      "Recall: 0.9712793733681462\n",
      "F1 Score: 0.970013037809648\n",
      "---------------------------LOR with Bernoulli Model----------------------\n",
      "Best parameters: {'alpha': 0.0001, 'learning_rate': 'constant', 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l1'}\n",
      "Accuracy: 0.9588785046728971\n",
      "Precision: 0.9664082687338501\n",
      "Recall: 0.9765013054830287\n",
      "F1 Score: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in range(no_of_datasets):\n",
    "    print(\"------------------------------dataset: \", dataset + 1, \"--------------------------------\")\n",
    "    train_matrices = [bow_matrices, bnouli_matrices]\n",
    "    test_matrices = [test_bow_matrices, test_bnouli_matrices]\n",
    "    for i in range(2):\n",
    "        data_matrix = train_matrices[i][dataset].copy()\n",
    "        if i == 0:\n",
    "            print(\"----------------------------SGD Classifier with BOW MODEL------------------------\")\n",
    "        else:\n",
    "            print(\"---------------------------SGD Classifier with Bernoulli Model----------------------\")\n",
    "       \n",
    "        \n",
    "        X_train = data_matrix[:,:-1]\n",
    "        Y_train = data_matrix[:, len(data_matrix[0])-1:]\n",
    "\n",
    "        Y_train = [y[0] for y in Y_train]   \n",
    "\n",
    "        test_matrix = test_matrices[i][dataset].copy()\n",
    "\n",
    "        X_test = test_matrix[:,:-1]\n",
    "        Y_test = test_matrix[:, len(test_matrix[0])-1:]\n",
    "\n",
    "        Y_test = [y[0] for y in Y_test]\n",
    "\n",
    "        sgd = SGDClassifier(eta0=0.1, random_state=42)\n",
    "\n",
    "\n",
    "        param_grid = {\n",
    "            'loss': ['log_loss', 'squared_hinge', 'perceptron'],\n",
    "            'penalty': ['l2', 'l1'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate': ['constant', 'optimal', 'adaptive'],\n",
    "            'max_iter': [100, 250]\n",
    "        }\n",
    "\n",
    "\n",
    "        grid_search = GridSearchCV(sgd, param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "\n",
    "        grid_search.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "        Y_pred = grid_search.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        precision = precision_score(Y_test, Y_pred)\n",
    "        recall = recall_score(Y_test, Y_pred)\n",
    "        f1 = f1_score(Y_test, Y_pred)\n",
    "\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 Score:\", f1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
